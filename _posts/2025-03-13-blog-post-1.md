---
title: "Navigating Stochasticity: Reinforcement Learning on Frozen Lake"
date: 2025-03-13
permalink: /posts/2025/03/blog-post-1/
tags:
  - machine learning
  - reinforcement learning
  - q-learning
  - frozen lake
  - stochasticity
---

<img src="/portfolio/images/blog/frozen_lake-1.gif" alt="Gif Frozen Lake" width="280px" style="float: right; box-shadow: 0 0 10px rgba(0, 0, 0, 0.3); margin-left: 25px; margin-bottom: 15px;">

Reinforcement Learning really shines when environments become *unpredictable*. In the present project, I'll explain the implementation of reinforcement learning to solve the [Frozen Lake environment](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/) from the OpenAI Gymnasium Library with a crucial twist: the slippery ice! </br> 
This introduces *stochasticity* to our environment, making it significantly more challenging and more representative of real-world problems.

<div style="clear: both;"></div>

FrozenLake presents itself with a grid-world where our agent must navigate from a starting point to a goal while avoiding holes in the ice. It's a perfect environment for demonstrating fundamental RL concepts in a simple yet visual way.

## The Frozen Lake Challenge

Our agent begins its journey at the top-left corner of an 4x4 grid representing a lake and must reach its goal at the bottom-right corner. The challenge being the holes in the ice, where stepping on them will result in failure. Our agent then needs to learn which path is safe by interacting with the environment. 

The agent can move in four directions: left, down, right and up. But here's the catch: when the environment parameter `is_slippery=True`, the intended action will only take place $$\frac{1}{3}$$ of the time. The other $$\frac{2}{3}$$ of the time, the agent will slide perpendicular to the intended direction. 

This added stochasticity is what makes our problem particularly interesting from a reinforcement learning perspective. Our agent needs to not only learn a direct path but also a robust one that accounts for the uncertainty of movement. 

## Q-learning: Adapting to Uncertainty

Q-learning is particularly well-suited for stochastic environment because it uses the expected value of actions, accounting for both the immediate reward and the uncertain future states. Here's how it works in simple terms:

1. The agent first observes the current state 
2. Based on its knowledge, it then chooses an action. 
3. It takes the action, observes the reward as well as the new state it is now in. 
4. It updates its knowledge within using the Q-learning update formula. It stores this new knowledge within a Q-table (16x4), grouping the calculated Q-values in each of the 16 states (the positions on the lake) that the agent could be in and 4 actions (up, down, left, right) that the agent could take. 
5. It then repeats this method until either the goal is reached or failure occurs.

**The Q-learning formula:**

<div align="center">
$$Q(s, a) \leftarrow Q(s, a) + \alpha \cdot [R + \gamma \cdot \max_{a'} Q(s', a')  - Q(s, a)]$$
</div>

Where:
- $$Q(s, a)$$ is the Q-value for state $$s$$ and action $$a$$
- $$\alpha$$ is the learning rate (how quickly new information overrides old information)  
- $$R$$ is the reward received after taking action $$a$$ in state $$s$$  
- $$\gamma$$ is the discount factor (how much future rewards are valued)  
- $$\max_{a'} Q(s', a')$$ is the maximum Q-value for the next state $$s'$$ across all possible actions $$a'$$

As the agent encounters different outcomes for the same state-action pair, it gradually builds a statistical understanding of the expected values, accounting for stochasticity in the environment.

## Implementing Q-learning

### Setup and initialization

We first start by creating the environment itself, specifying that it becomes stochastic, and then inialize a version or our Q-table where the agent will store the outputs of the Q-function in the event that it is training. 

If the agent is testing, we then load a pre-trained Q-table from a `lake.pkl` file. 

We then define the hyperparameters which will be used by the Q-learning algorithm.

```python
# Defining the function of the problem
def run(episodes, is_training, render=False):

  # Creating the environment
  env = gym.make('FrozenLake-v1', map_name='4x4', is_slippery=True, render_mode='human' if render else None)

  # Q-table initialization (training/testing)
  if(is_training):
    q = np.zeros((env.observation_space.n, env.action_space.n))
  else:
    f = open('lake.pkl', 'rb')
    q = pickle.load(f)
    f.close()

  # Hyperparameter configuration
  learning_rate_alpha = 0.9
  discount_factor_gamma = 0.9
  epsilon = 1
  epsilon_decay = 0.0001
  random_number = np.random.default_rng()
  rewards_per_episode = np.zeros(episodes)
  ```

In this case, we choose to keep the learning rate and exploration decay to lower values in order to give the agent more time to learn the probabilistic nature of the environment.

## Training/testing loop

Following is the heart of the implementation. For each episode the agent will first start at an initial state and choose actions using an *epsilon-greedy policy* which blances exploration (trying out random actions) and exploitation (taking actions based on what it has learned). Based on the rewards received, the agent will update its Q-table and gradually reduce exploration (epsilon value determining the randomeness of actions) as it learns. It will then record successfull episodes.

This *epsilon-greedy strategy* is crucial as it allows our agent to explore the environment in earlier episodes (when epsilon is high) and exploit its knowledge in later episodes (when epsilon decreases)

```python
# Initializing the loop
for i in range(episodes):
  state = env.reset()[0]
  terminated = False
  truncated = False

  while(not terminated and not truncates):
    # Action selection (epsilon-greedy strategy)
    if is_training and random_number.random() < epsilon:
      action = env.action_space.sample()
    else:
      action = np.argmax(q[state,:])
    
    # Environment interaction
    new_state, reward, terminated, truncated, _ = env.step(actions)

    # Q-table update (when learning)
    if is_training:
      q[state, action] = q[state, action] + learning_rate_alpha 
      * (reward + discount_factor_gamma * np.max(q[new_state]) - q[state, action])

    state = new_state

  # Update learning parameters as time moves on to decrease exploration
  epsilon = max(epsilon - epsilon_decay, 0.01)
  if(epsilon <= 0.1):
    learning_rate_alpha = 0.1
  
  # Mark the current episode as successfull if a reward was received
  if reward == 1:
    rewards_per_episode[i] = 1
```

In this specific case, we ensure a minimum epsilon of $$0.01$$ to ensure some level of exploration continues even in later stages of training, which is important in stochastic environments where the agent needs to continually reassess the value of different paths in order to avoid getting stuck in local optima and better adapt to the inherent randomness of the environment.

### Performance analysis

This section now calculates and visualizes the agent's learning progress by tracking the rolling sum of rewards (completions of the environment) over time. The resulting plot will demonstrate how quickly our agent improves.

```python
sum_rewards = np.zeros(episodes)
for t in range(episodes):
  sum_rewards[t] = np.sum(rewards_per_episode[max(0, t-100):(t+1)])

plt.figure(figsize=(8, 5), facecolor='none')
plt.plot(sum_rewards, color='#1976D2', linewidth=1.5)
plt.title('FrozenLake-v1 Performance')
plt.xlabel('Episode')
plt.ylabel('Cumulative Reward')
plt.grid(True, linestyle='--', alpha=0.3)
plt.tight_layout()

plt.savefig('lake.png', transparent=True, dpi=100)
plt.close()
```

### Model persistence

We then finally save our trained Q-table in order to reuse it later without the need to retrain the agent.

```python
if is_training:
  f = open('lake.pkl', 'wb)
  pickle.dump(q, f)
  f.close()
```

## Results and analysis

After training the agent for $$15,000$$ episodes, we can observe the following results, when plotting the comulative rewards per $$100$$ episodes as a function of total  number of episodes. The below graphs compare both a stochastic as well as a deterministic environment for which the argument `is_slippery=False` was passed in.

<div style="display: flex; justify-content: space-between; margin-bottom: 20px; flex-wrap: wrap;">
  <div style="width: 48%; min-width: 300px; margin-bottom: 15px;">
    <img src="/portfolio/images/blog/lake_4x4_noslip.png" alt="Results Frozen Lake No Slip" style="width: 100%; box-shadow: 0 0 10px rgba(0, 0, 0, 0.3);">
    <p style="text-align: center; font-style: italic; font-size: 0.9em; margin-top: 5px;">Deterministic environment</p>
  </div>
  <div style="width: 48%; min-width: 300px; margin-bottom: 15px;">
    <img src="/portfolio/images/blog/lake_4x4_slip.png" alt="Results Frozen Lake Slip" style="width: 100%; box-shadow: 0 0 10px rgba(0, 0, 0, 0.3);">
    <p style="text-align: center; font-style: italic; font-size: 0.9em; margin-top: 5px;">Stochastic environment</p>
  </div>
</div>

In the case of the stochastic environment (right), the learning curve shows a much more gradual improvement than when compared with a deterministic non-slippery environment (left). 

- Initial learning is slower: the agent agent needs more episodes to start seeing consistent success.
- Performance plateaus: there often are periods of stagnation where the agent struggles to find a reliable path.
- Final performance is lower: even a well-trained agent will occasionally fail because of the random nature of the environment.

There's also a clear difference in the behaviors of both trained agents as can be seen when rendering both environments:

<div style="display: flex; justify-content: space-between; margin-bottom: 20px; flex-wrap: wrap;">
  <div style="width: 48%; min-width: 300px; margin-bottom: 15px;">
    <img src="/portfolio/images/blog/fl_4x4_noslip.gif" alt="Gif Frozen Lake No Slip" style="width: 100%; max-width: 300px; display: block; margin: 0 auto; box-shadow: 0 0 10px rgba(0, 0, 0, 0.3);">
    <p style="text-align: center; font-style: italic; font-size: 0.9em; margin-top: 5px;">Deterministic environment</p>
  </div>
  <div style="width: 48%; min-width: 300px; margin-bottom: 15px;">
    <img src="/portfolio/images/blog/fl_4x4_slip.gif" alt="Gif Frozen Lake Slip" style="width: 100%; max-width: 300px; display: block; margin: 0 auto; box-shadow: 0 0 10px rgba(0, 0, 0, 0.3);">
    <p style="text-align: center; font-style: italic; font-size: 0.9em; margin-top: 5px;">Stochastic environment</p>
  </div>
</div>

<div style="clear: both; margin-bottom: 20px;"></div>

In the case of the deterministic non-slippery environment, the agent finds only one optimal path which maximizes the reward in the minimal number of required steps.

In contrast, in the stochastic slippery environment, the agent finds a very different strategy by favoring moving in the direction of the edges and betting on the $$\frac{1}{3}$$ probability of being pushed in the opposite direction in order to attain its goal rather than to risk being pushed in a hole. In favors a more conservative but seemingly safer policy with a considerably larger amount of steps than its deterministic version.

This illustrates the fundamental challenge of reinforcement learning in stochastic environments: the agent must learn a policy that maximizes epected rewards over time rather than finding a single correct path.